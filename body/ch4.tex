% !Mode:: "TeX:UTF-8"

\chapter{拉萨方言语音识别}
拉萨方言作为语料匮乏语言的一种，想训练其语音识别系统，难点在于收集足够多的带文本标签的语音数据，同时也缺少足够的藏语语音学和语言学的相关知识。这些限制条件使得拉萨方言语音识别的研究进展缓慢，远远落后于目前英语和汉语的语音识别技术水平。在本研究中，我们录制了一个小规模的拉萨方言数据库，从头开始搭建了拉萨方言离线识别系统。在语音数据不充足的条件下，调查了拉萨方言声调信息对语音识别性能的影响。由于拉萨方言目前还没有一个公认的声调系统，因此本论文中，我们采用了四个声调的声调模式，设计了带调的音素集合，并且在特征层面加入了声调相关的信息。实验结果表明，利用声调信息之后，识别的准确率相对提升16.0\%。以下是拉萨方言语音识别系统实验的详细介绍。
\section{拉萨方言语音识别研究现状}
拉萨方言属于西藏中部方言的一种，使用者包括拉萨以及周边地区的居民。由于拉萨在政治和文化方面的重要性，拉萨方言相关的研究近些年开始受到越来越多的关注。现有的拉萨方言相关研究工作，其中一部分为拉萨方言声学模型的研究。\cite{yao2009research}是较早的使用DTW做拉萨方言孤立词识别的，\cite{li2012research}\cite{li2014large}是使用传统GMM做拉萨方言连续语音识别的。很明显，以上的研究所用技术已经落后于目前流行的方法。\cite{wang2014cross}\cite{zhao2015shared}关注的是如何使用神经网络的共享隐层解决拉萨方言训练数据不足的问题。虽然\cite{wang2014cross}\cite{zhao2015shared}等最新的研究使用了流行的深度学习技术，但是很少有相关工作提到如何使用拉萨方言本身的特性去提高识别性能。

拉萨方言属于单音节的有调语言，每个拉萨方言的单字都是一个带调的音节，声调在区分同音字上扮演着很重要的角色，尤其是在缺少较强的上下文信息的情况下。然而，很少有研究提到如何利用拉萨方言的声调去提高系统的识别性能。如果能对拉萨方言的声调建立准确模型，将会在很大程度上提高识别系统的准确度。
\section{拉萨方言数据库及发音字典}
藏语拉萨方言数据库由天津大学认知计算与应用重点实验室(CCA)与中国社会科学院民族学与人类学研究所合作录制，共包含13名男性发音人和10名女性发音人。发音人均是以拉萨方言为母语的中央民族大学本科生。每位发音人录制相同的3,100句拉萨口语音素平衡句，句子的平均时长为3.2秒。录制环境为安静的办公室环境。音频信号的获取采用单声道、16KHz采样率、16bit量化，保存为.wav格式的音频文件。数据经过人工校对，剔除掉不合格的音频数据，最后可用的数据总时长为35.92小时。将数据库分为三个部分，其中训练集数据用作模型训练，包含7名女性发音人和10名男性发音人，共36,090个句子，总时长为31.9小时；测试集用作模型测试，包含3名女性发音人和3名男性发音人，共2,664个句子，总时长为2.41小时；开发集用于选择模型参数，其发音人和训练集发音人一致，共1700个句子，总时长为1.51小时。训练集和测试集发音人和发音句子没有交叉，保证了模型测试结果的准确性。

发音字典由合作单位中国社会科学院民族学与人类学研究所提供，字典采用声韵母组合的规则，共包含29个声母，48个韵母。字典条目为2,100。基本涵盖了所有藏语拉萨方言数据库中出现的藏文字。图表~\ref{fig:tibetanphoneme}~是发音字典所用声韵母集合。
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{tibetanphoneme}
\caption{拉萨发音字典声韵母集合}\label{fig:tibetanphoneme}
\vspace{\baselineskip}
\end{figure}

\section{拉萨方言语音识别基准系统}
为了验证声调特征的有效性，我们首先搭建了基准系统。在搭建基准系统之前，我们尝试了两种不同的DNN声学模型框架，一种是DNN-HMM\cite{dahl2012context}；另外一种是Tandem方法\cite{hermansky2000tandem}。其中DNN-HMM使用深度神经网络代替传统的GMM模型，用来计算上下文相关的状态发射概率，使用HMM对时序关系建模；而Tandem方法是使用带有bottle-neck层的神经网络做为特征提取的手段，将bottle-neck层的输出与传统的MFCC特征拼接在一起作为最后的特征，并用这些特征训练GMM-HMM模型。根据以往研究人员的经验，Tandem方法一般能够达到DNN-HMM方法的效果，不过要比DNN-HMM实现起来稍微繁琐复杂。在本论文中，我们在语料不是很充足的前提下，分别尝试了这两种方法，并对结果进行了分析比较。
\subsection{CD-DNN-HMM}
DNN-HMM方法框架属于深度神经网络在语音识别应用中的一种。DNN-HMM利用了DNN模型强大的表征能力，同时也保留了HMM模型对序列建模的能力。当DNN的输出单元对应的不是单音素模型(monophone)，而是捆绑后的三音子模型(senone)时，DNN-HMM就被称为是CD-DNN-HMM(context-dependent-DNN-HMM)。与传统的GMM-HMM模型相比，DNN-HMM使用强大的神经网络模型计算HMM对应的发射概率。很多研究表明，CD-DNN-HMM在很多大词汇量连续语音识别的任务上都要强过GMM-HMM。由于CD-DNN-HMM与GMM-HMM模型共享senones和HMM模型，所以第一步需要训练GMM-HMM，并以GMM-HMM为基础去训练CD-DNN-HMM。例如，训练集里每个句子的senones的切分信息都是由GMM-HMM模型生成的。

在本论文中，对于GMM-HMM系统，输入特征首先经过谱均值方差归一化(CMVN)处理，接下来使用处理后的特征训练单音素模型(monophone system)，再接着使用单音素模型和维特比算法，对训练数据里的每个句子做强制对齐，得到音素的切分信息，最后，使用得到的切分信息训练三音素模型(triphone system)。在训练三音素模型的过程中，使用了线性判别分析(LDA)和最大似然线性变换(MLLT)等特征变换方法提高模型的性能。

对于CD-DNN-HMM模型，其模型结构为六个隐层，每个隐层2048个节点。训练DNN的输入特征为当前帧加前后五帧，总共11帧的MFCC参数。DNN的初始化参数通过预训练受限玻尔兹曼机(restricted RBM)得到。在预训练步骤完成后，训练阶段采用随机梯度下降算法(SGD)，训练的标签信息通过GMM-HMM对训练句子做强制对齐得到。训练过程中，特征空间最大似然线性回归(fMLLR)方法用来做说话人归一化，消除不同说话人对识别结果的影响。训练阶段，90\%的训练数据用来作为模型训练数据，10\%的训练数据作为验证集数据，用来选择学习速率等模型参数。整个的CD-DNN-HMM模型训练过程如图~\ref{fig:figure3}~所示。
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figure3}
\caption{CD-DNN-HMM训练过程}\label{fig:figure3}
\vspace{\baselineskip}
\end{figure}

\subsection{Tandem}
Tandem方法是将深度神经网络模型与传统的GMM-HMM模型融合的一种方式，在DNN-HMM模型中，DNN的隐层用做非线性特征变换，输出层(通常为softmax层)用做分类器。而在Tandem方法中，DNN模型只用做特征提取，最后的声学模型还是传统的GMM-HMM。而DNN提取的特征并不直接使用输出层的输出，而是用一个节点数明显少于其它隐层的中间隐层的激励函数的输出作为输出特征。使用中间隐层输出的好处是，中间隐层的节点个数独立于输出层节点个数，可以随意设置，这样方便控制特征的维度。一般情况下，DNN的bottle-neck层的输出会拼接到MFCC或者PLP特征上，并且使用PCA或者HLDA降维并去除相关性，之后再训练GMM-HMM。

在本论文中，我们使用一个带有bottle-neck层的前馈神经网络，总共有三个隐层，中间的那个隐层作为bottle-neck层。bottle-neck层的节点数为42，其余隐层节点数为1024。隐层节点的激励函数使用双曲正切函数。整个Tandem方法的详细过程如图~\ref{fig:figure4}~所示。
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure4}
\caption{Tandem方法实现过程}\label{fig:figure4}
\vspace{\baselineskip}
\end{figure}

\subsection{两种建模方法的音素级识别结果}
为了单纯地比较两种声学建模方法的准确率，而不用过多考虑语言模型的影响，我们首先使用音素级别错误率作为评价指标。此时用到的语言模型的训练数据全部来自于训练数据的音素标注。下表~\ref{tab:table2}~是两种声学建模方法的音素级别的错误率。
\begin{table}[htbp]
\caption{两种声学建模方法对应音素级别错误率}\label{tab:table2}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{cc}
\toprule[1.5pt]
Acoustic Model & PER \\
\midrule[1pt]
 CD-DNN-HMM & 20.84\% \\
Tandem Approach & 21.48\% \\
\bottomrule[1.5pt]
\end{tabular}
\vspace{\baselineskip}
\end{table}

为了更直观地表示两种声学建模方法的识别错误类型，本文在这里对两种方法分别画出了音素混淆矩阵。$x$轴代表音素的标签，$y$轴代表音素的识别结果。颜色越亮代表音素$x$被识别成$y$的概率越大。对角线上的每个点表示音素被正确识别的概率。图~\ref{fig:figure5}~和图~\ref{fig:figure6}~分别是CD-DNN-HMM和Tandem对应的混淆矩阵：
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure5dnn}
\caption{CD-DNN-HMM音素识别结果混淆矩阵}\label{fig:figure5}
\vspace{\baselineskip}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure6tandem}
\caption{Tandem音素识别结果混淆矩阵}\label{fig:figure6}
\vspace{\baselineskip}
\end{figure}

通过比较两个混淆矩阵，我们可以看到两种方法识别结果的错误类型大体一致，在DNN-HMM中出现的比较严重的错误类型，在Tandem方法中也同样出现了。根据以上的识别结果和错误类型分析，我们决定选用结果稍好的CD-DNN-HMM方法作为基准系统的声学模型训练方法。
\subsection{CD-DNN-HMM基准系统}
选定了CD-DNN-HMM作为声学模型之后，为了搭建实用的语音识别系统，还必须使用有效的语言模型。因为我们目前没有较好的藏文分词工具，所以系统使用字级别的语言模型并采用字级别的错误率作为评价标准。由于目前训练声学模型使用的数据为拉萨口语句子，很难搜集和音频数据主题相关的口语文本语料。目前使用的语言模型的训练数据包含两个部分，一部分是从维基百科上爬取的藏语文本数据；另外一部分是藏族五省区中学课本的一部分。总共包括14,430个藏语句子。语言模型使用的是三元语法模型，平滑方法选用的是Kneser-Ney方法。表~\ref{tab:table3}~是基准系统的字级别错误率：
\begin{table}[htbp]
\caption{基准系统字级别错误率}\label{tab:table3}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{ccc}
\toprule[1.5pt]
Acoustic Model & CER & Details \\
\midrule[1pt]
 CD-DNN-HMM & 33.97\% & 8,190/24,108 [246 ins; 154 del; 7,790 sub] \\
\bottomrule[1.5pt]
\end{tabular}
\vspace{\baselineskip}
\end{table}

通过检查识别结果，我们发现有很多由于同音字造成的替换错误。图~\ref{fig:figure7}~是测试集中随机挑选的一个句子及其对应的识别结果，其中两处替换错误都被红色标识出来了。对于第一处替换错误，原文字和替换文字是一对儿同音字，组成两个字发音的声韵母在发音字典里是相同的，因此导致了识别错误。同音字替换错误本来是可以通过语言模型解决的，但是由于目前使用的语言模型的训练数据缺少主题相关的口语语料，导致语言模型的精度较差。这也是目前拉萨方言语音识别普遍存在的问题。然而，图~\ref{fig:figure7}~中的同音字替换错误的例子，两个同音字的声调其实是不同的，所以我们可以利用拉萨方言的声调信息来提高识别的准确度。
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure7}
\caption{标号为TF207\_0002的句子，上一行为文本，下一行为识别结果}\label{fig:figure7}
\vspace{\baselineskip}
\end{figure}

\section{拉萨方言声调系统}
在利用声调信息之前，我们首先需要了解清楚拉萨方言的声调。和普通话的声调类似，拉萨方言的声调也是具有词汇意义的，即声调可以用来区分同音字。为了能够对声调建模，我们需要知道拉萨方言的声调总共有多少种类型，每种类型的声调曲线形状是什么样的。
\subsection{拉萨方言的四个声调类型}
藏语属于汉藏语系，缅藏语组。藏语有三大方言，分别是：拉萨方言、康巴方言和安多方言。其中康巴方言和拉萨方言被认为是有调语言，而安多方言被认为是无调语言。在这三大方言中，拉萨方言的声调特性更加丰富。然而，有关拉萨方言究竟该划分为多少个声调还存在争议。其中，已经被广泛认可的是拉萨方言中存在高低调的区别。根据\cite{hu2010lhasa}所述，拉萨方言的声调韵律在很大程度上由音节的类型决定。从音韵学角度来说，拉萨方言可以被认为共有两个、四个、六个，甚至八个声调。声调个数的不确定性给利用声调信息带来了困难。在本论文中，我们的目的是找到一个能够有效利用声调信息提高识别准确率的方法。因此，我们基于\cite{Yudaoquan}设计了四个声调的拉萨方言声调系统。根据音韵学原理，我们使用五度值的方法\cite{chao1980system}来表示不同的声调类型。四个声调可分别表示为：55(高平调)、13(升调)、51(降调)、132(升降调)。由于拉萨方言的韵母是携带声调的主要部分，因此，我们将音素表中的所有韵母扩充成四个不同声调的韵母，声母保持不变。最后得到的音素集合总共包含29个声母和192个带调韵母。图~\ref{fig:figure8}~和图~\ref{fig:figure9}~展示的是从训练集中随机挑选的一个句子，图~\ref{fig:figure8}~是女性发音人录制的，图~\ref{fig:figure9}~是男性发音人录制的。每个图中均包含声音波形、语谱图、音素标注。语谱图中的细实线表示的是声调曲线，音素标注使用的是带调的音素集合。
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure8}
\caption{女性发音人基频曲线及音素标注}\label{fig:figure8}
\vspace{\baselineskip}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure9}
\caption{男性发音人基频曲线及音素标注}\label{fig:figure9}
\vspace{\baselineskip}
\end{figure}

\section{声调特征提取}
因为我们是通过声调曲线轮廓去区分不同声调的，所以对声调建模实际上是pitch-tracking问题。pitch并不是纯粹客观的物理性质，它是声音的主观心理声学属性。而基频F0(Fundamental frequency)作为周期性信号的固有属性，与人对声音的pitch感知联系十分紧密。因此，几乎所有的pitch-tracking算法都是去估计声音的基频值\cite{kleijn1995robust}。在本论文中，我们总共使用了两种提取基频值的方法，一个是SAcC方法\cite{lee2012noise}，另外一个是Kaldi-pitch方法\cite{ghahremani2014pitch}。这两个方法在pitch-tracking标准测试集Keele数据集\cite{plante1995pitch}上都表现出了很好的性能。
\subsection{SAcC方法}
SAcC全称是Subband Autocorrelation Classification的缩写，该方法首先将音频信号$a[n]$通过一个带有48个带通滤波器的滤波器组，得到48个子带信号$b_k[n],k=1,2,...,48$。接着计算每个子带信号归一化的自相关系数(Normalized Autocorrelation)，然后使用PCA对自相关系数降维，最后用降维后的自相关系数训练多层感知机(MLP)模型。MLP的输出可以用来估计给定观测下，pitch为某一值的后验概率。最后，基于MLP的输出，使用Viterbi算法搜索每一帧最佳的基频值，同时判断该帧是否为静音帧。详细的SAcC方法描述可以参考\cite{lee2012noise}。
\subsection{Kaldi-Pitch方法}
Kaldi-Pitch方法\cite{ghahremani2014pitch}属于数据驱动方法，是对经典的基频提取方法Getf0方法\cite{kleijn1995robust}的改进版本。Getf0方法通过计算归一化的互相关函数得到音频信号每一帧的一系列基频备选值，之后使用DP(Dynamic Programming)算法，根据局部和全局的信息，得到每一帧的最佳基频值，对于静音帧，直接将基频值赋成0。Kaldi-Pitch方法与Getf0方法的最大区别在于，Kaldi-Pitch并不直接决定每一帧是否为静音帧。对于静音帧，Kaldi-Pitch方法会将其基频赋成一个非零值，目的是保持基频曲线的连续性。但同时，Kaldi-Pitch方法引入了发声概率这个变量，用来描述该帧是非静音帧的概率。\cite{ghahremani2014pitch}的实验表明，用连续的基频曲线加上每一帧的发声概率作为声调信息，结合MFCC特征，在BABEL这个标准数据集上对多个语种的语音识别都取得了不错的识别效果。和Getf0方法一样，Kaldi-Pitch用来计算基频备选值的方法同样是基于归一化的互相关函数。假设$x_p,p=0,1,2,...$是采样后的语音信号，采样周期$T=1/F_s$，时间窗长度为$\omega$，窗移长度为$t$，每个时间窗内的采样点个数$n=\omega/T$，窗与窗之间相距的采样点个数$z=t/T$。那么，对于第$i$帧信号，延迟系数为$k$时，其对应的互相关系数$\phi_{ik}$的计算公式如下：
\begin{eqnarray}
  \phi_{ik}=\frac{\sum_{j=m}^{m+n-1} x_j x_{j+k}}{\sqrt{e_m e_{m+k}}} \\
  e_j=\sum_{i=j}^{j+n-1} x_i^2
\end{eqnarray}
其中，$m=i*z$，i代表帧的标号，k代表延迟系数。

对于每一帧信号$i$，NCCF的局部极大值点就是基频值的备选。根据上述公式，我们可以知道$-1.0\leqslant\phi_{ik}\leqslant1.0$，并且当延迟系数$k$接近基频的正整数倍时，$\phi_{ik}$接近于1.0。

假设c是给定某一帧的NCCF值，其绝对值$a=abs(c)$。以下是通过分析Keele数据库的$log(\frac{count(voiced)}{count(unvoiced)})$曲线得到的公式：
\begin{equation}h=-5.2+5.4 e^{7.5 (a-1)}+4.8 a-2 e^{10 a}+4.2 e^{20 (a-1)}\end{equation}
此处，h近似等于$log(\frac{count(voiced)}{count(unvoiced)})$，$p=\frac{1}{1+e^{-h}}$近似等于当前帧是非静音帧的概率。
\subsection{声调相关特征}
在本论文中，提取基频相关特征使用两种不同的pitch-tracking方法，分别是SAcC方法和Kaldi-Pitch方法。由SAcC方法计算的基频值只在非静音段不为零，造成基频曲线不连续，我们可以选择是否做插值平滑处理。所以，总共会有三种不同的基频相关特征，分别是Kaldi-F1、Interpolate-SAcC-F2、Raw-SAcC-F3。以下是对这三种特征的详细介绍。

尽管基频只在音频的有声段才有定义，但由于Kaldi-Pitch方法并不会直接将无声段的基频值赋为0，从而保证了基频曲线的连续性，因此不需要做额外的插值处理。Kaldi-Pitch方法的输出包括基频的对数值还有每一帧为非静音帧的概率。为了提取基频曲线的动态特征，我们使用当前帧的前两帧和后两帧计算了基频的delta值。因此，对于每一帧音频数据，总共有三维的基频相关特征，直接将这三维特征拼接到MFCC特征后面，得到Kaldi-F1特征。

对于SAcC方法，由于它直接将无声段的基频值赋成0，为了避免特征的不连续导致识别性能下降，此处对基频曲线做了插值和平滑处理，具体过程类似于\cite{lei2006improved}。选用的插值方法是piecewise cubic Hermite interpolating polynomial (PCHIP)\cite{fritsch1980monotone}。选用的平滑方法是5-point moving average。与Kaldi-Pitch方法类型，SAcC方法同样提供每一帧为非静音帧的概率。因此，使用SAcC方法，对于每一帧，同样也是三维的基频相关特征，包括基频的对数值、非静音帧的概率、基频的delta值。将这三维的特征拼接到MFCC特征后面，得到Interpolate-SAcC-F2特征。为了比较使用连续的基频曲线和不连续的基频曲线对声学建模的效果，此处还设置了Raw-SAcC-F3特征，同样也是将基频特征直接拼接到MFCC特征后面，但此处的基频相关特征只包括未处理的基频值F0、非静音帧的概率。
\section{加入声调信息的识别系统}
本论文中，引入声调信息主要通过两种方式，一种是将音素集合中的韵母扩充成带调韵母的形式；另外一种就是使用声调相关特征训练声学模型。为了评价带调音素集合和声调相关特征的有效性，此处设置了四组对比试验。首先是使用带调的音素集合加上MFCC特征训练声学模型；接下来将音素集合变为带调音素集合，将MFCC分别替换为Kaldi-F1、Interpolate-SAcC-F2以及Raw-SAcC-F3。声学模型和语言模型的训练方法和参数配置与4.3.4中的基准系统相同。表~\ref{tab:table4}~为四个不同的声学模型以及基准系统在测试集上的字级别错误率。
\begin{table}[htbp]
\caption{四个不同声学模型及基准系统字级别错误率}\label{tab:table4}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{cc}
\toprule[1.5pt]
Different Models & CER \\
\midrule[1pt]
Non-tonal Phone + MFCC & 33.97\% \\
Tonal Phone + MFCC & 31.91\% \\
Tonal Phone + Kaldi-F1 & 30.20\% \\
Tonal Phone + Interpolate-SAcC-F2 & 31.30\% \\
Tonal Phone + Raw-SAcC-F3 & 31.91\% \\
\bottomrule[1.5pt]
\end{tabular}
\vspace{\baselineskip}
\end{table}

从表中的识别结果可以看出，通过使用带调的音素集合，即使是特征层面不做任何改变，相比基准系统能获得相对6.1\%的性能提升。当使用带调音素集合，同时将MFCC换成Kladi-F1特征时，累积的性能提升为11.1\%。当把Kaldi-F1特征换成Interpolate-SAcC-F2特征时，性能有所下降，但依然要好于带调音素集合加上MFCC特征的情况。\cite{ghahremani2014pitch}中展示了使用Kaldi-Pitch方法得到的特征相比于SAcC方法要更适合用来训练SGMM模型，而本论文中的结果表明，对于DNN模型而言，Kaldi-Pitch方法依然要优于SAcC方法。另外，当直接使用原始的基频值加上非静音帧概率，而不做插值和平滑处理时，识别结果和使用带调音素结合加上MFCC特征相同。这表明此处的特征层面的声调信息对识别没有起到帮助。从识别结果中可以看到，利用声调信息确实可以提高识别准确度，但是目前总体的识别准确度并不是很高。主要原因是目前训练语言模型的训练数据与测试集的口语句子不匹配，导致语言模型性能较差。
\subsection{系统融合}
除了尝试不同的声调特征来训练声学模型，本论文还尝试了使用基于Minimum Bayes Risk(MBR)的lattice combination\cite{xu2011minimum}，此方法将不同系统模型对同一个句子得到的lattice融合为一个统一的lattice。此处总共做了四种不同的系统融合，表~\ref{tab:table5}~为系统融合后得到的识别结果。
\begin{table}[htbp]
\caption{不同类型的系统融合字级别错误率}\label{tab:table5}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{cc}
\toprule[1.5pt]
Combining Different Systems & CER \\
\midrule[1pt]
Kaldi-F1 + Interpolate-SAcC-F2 & 28.92\% \\
Kaldi-F1 + Raw-SAcC-F3 & 28.97\% \\
Interpolate-SAcC-F2 + Raw-SAcC-F3 & 29.87\% \\
All three systems & 28.54\% \\
\bottomrule[1.5pt]
\end{tabular}
\vspace{\baselineskip}
\end{table}

从上表中的结果可以看出，将三个不同的系统融合在一起能得到最高的识别性能，Kaldi-F1系统和Interpolate-SAcC-F2系统之间的融合要优于Kaldi-F1系统和Raw-SAcC-F3系统之间的融合。无论哪一种系统融合方式，其结果都要优于任何单独的系统。基于以上结果，可以推测，不同的系统模型之间可以优势互补，尤其是当两种模型使用不同的基频提取方法时，融合后的效果提升更加明显。
