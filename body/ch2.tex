% !mode:: "tex:utf-8"

\chapter{背景介绍}
本章主要介绍语音识别的一些基本概念，包括前端的特征提取、声学模型、语言模型、解码器、性能评价指标，另外简单介绍了本研究涉及到的语音识别工具箱——Kaldi。
\section{自动语音识别}
语音识别，顾名思义，是要把人的声音转化成文本，目标是在给定声音的前提下找到最有可能的文本序列。语音识别系统的结构可以用下图表示：

{\color{red}{\large 补充语音识别系统结构图}}

从图中可以看出，语音识别系统总共包含五个部分，输入的音频首先经过语音识别前端的特征提取部分，现假设语音长度为T，那么经过特征提取会得到一系列固定长度的频谱特征向量$X_{t}$，t=1,2,...T。语音识别的输出是一连串字符$\omega_{k}$，k=1,2,...K。$\omega$是系统认为最能匹配输入音频的文字序列。因此语音识别的目标可以表示为找到对应的$\omega$使之满足
\begin{equation}\widehat{\omega} = \underset{\omega}{\operatorname{argmax} \frac{P(X|\omega)*P(\omega)}{P(X)}}\end{equation}$P(\omega|X)$
很难直接计算，根据贝叶斯公式
\begin{equation}P(\omega|X)=\frac{P(X|\omega)*P(\omega)}{P(X)}\end{equation}
对于给定的输入X，P(X)对所有$\omega$均为定值，因此
\begin{equation}\widehat{\omega} = \underset{\omega}{\operatorname{argmax} P(X|\omega)*P(\omega)}\end{equation}
其中，$P(X|\omega)$代表声学模型，$P(\omega)$代表语言模型。解码器根据声学模型和语言模型对$\omega$的评分，搜索所有可能的$\omega$，得到最优解。以下是语音识别系统各部分的详细介绍。
\subsection{特征提取}
对于语音识别系统而言，声学信号作为用户的唯一输入，需要承载用于识别的所有信息。如果仅对声学信号在时域上的波形进行分析，很难从中提取出对识别有用的特征，因为即使同一个人说同样一段话，单从波形上看都会有很大差别。然而，一个受过训练的人，可以通过语谱图区分不同的元音，因为元音的频率成分相对固定，不同元音的频谱图会有明显的差别。根据这一特性，我们可以在频域上对信号进行分析，从声学信号中提取与频率有关的特征，用来作为识别系统的输入。目前语音识别系统常用的声学特征包括：梅尔频率倒谱系数（MFCC）、感知线性预测(PLP)、Filter-bank等。下面以语音识别中常用的MFCC参数为例，详细介绍声学参数的提取过程。

梅尔频率倒谱系数(MFCC)作为语音识别中比较常用的声学特征参数，其原理是模仿人耳的听觉机理，将以赫兹为单位的频率变换成梅尔频率，使用在梅尔刻度上等距分布的梅尔滤波器组搜集不同频段的能量，通过逆离散傅里叶变换（IDFT）计算倒谱系数，实现声源和滤波器的分离，并降低不同维度特征之间的相关性。最后加入能量以及帧与帧之间的变换的信息。计算梅尔频率倒谱系数的详细过程如下：模拟信号经过采样和量化，转换为数字信号x[n]，n对应采样时刻。接下来对x[n]加窗，由于声学特征是用来区分语音信号中不同音素的，所以我们需要分析大致对应每个音素长度的部分的波形，这就需要对整个信号做加窗处理，窗口外部的信号全部设为零，只保留窗口内部的信号。一般情况下窗长设为25ms，每10ms 向前移动一个时间窗。这样每段音频都转化成了相互之间有重叠部分的固定长度的数字向量。MFCC提取过程中普遍使用的窗函数为Hamming Window，其公式为:
\begin{equation}
  f(x)=
    \begin{cases}
      0.54 - 0.46\cos(\frac{2{\pi}n}{L}) & 0\leq x\leq L-1 \\
      \text{0} &\quad\text{otherwise.} \
    \end{cases}
\end{equation}
接下来是对加窗后的数字信号做离散傅里叶变换，离散傅里叶变换的目的是计算信号在不同频段所包含的能量。其公式为:\begin{equation}X[k]=\sum_{n=0}^{N-1} x[n]{e^{-j\frac{2\pi}{N}k{n}}}\end{equation}DFT的N个输出对应N个离散频带，X[k]为复数，代表当前频率成分的幅值和相位。FFT是实现离散傅里叶变换的高效算法，但限制是N必须为2的正整数次幂。最后FFT 的输出表示每个频段的能量。前文已经提到过，MFCC是基于人耳的听觉感知机理设计的，人耳对频率的感知是非线性的，超过1000Hz时，人类对频率的变换越来越不敏感。研究表明，在特征提取时，通过建模人类听觉的这种特性，能够提高识别系统的性能[Davis and Mermelstein]。MFCC通过引入梅尔刻度来模拟人耳的机制，梅尔刻度[Stevents and Volkmann]是描述声调的单位，两组信号如果在声调的感知上是等距的，那么它们的梅尔频率也是等距的。以赫兹为单位的频率与梅尔频率之间的关系可以使用如下公式表示：\begin{equation}mel(f)=1127\ln(1+\frac{f}{700})\end{equation}计算MFCC时，通过放置一组三角滤波器来收集不同频带的能量，三角滤波器在Mel刻度下是等宽均匀排布在整个频率范围内的。接着对每个梅尔滤波器的输出取对数，这样可以减弱特征对输入变化的敏感性，比如发音人与麦克风之间距离的变化。如果直接将梅尔滤波器输出取对数后的值作为特征，因为各个滤波器输出值之间相关性不为零，会造成后续训练高斯混合模型时协方差矩阵无法使用对角阵，所以需要进一步处理。倒谱（cepstrum）是对频谱取对数之后做逆傅里叶变换，倒谱可以实现声源和滤波器分离，并去除特征不同维度之间的相关性，因此取倒谱系数的前12维作为MFCC的特征。由于每一帧的能量和当前帧所属音素有关，可以将能量作为MFCC的一个维度：\begin{equation}E(m) = \sum_{t=t_1}^{t_2} x^2[t]\end{equation}
其中m表示帧的标号，$t_1$和$t_2$分别代表帧的起始时刻和终止时刻。除了能量之外，前后帧之间的变化信息也有助于识别不同的音素，所以MFCC一般还会加入倒谱系数每一维的一阶差分和二阶差分，以及能量的一阶差分和二阶差分。最终，从每一帧信号中提取出39维的MFCC向量，用于训练声学模型和识别。
\subsection{声学模型}
在隐马尔科夫模型（HMM）被用到语音识别之前，人们使用Dynamic Time Warping(DTW)来搭建最简单的语音识别器。DTW用来计算测试音频和模板音频之间的距离，假设需要识别0-9十个数字，那么就需要有0-9这十个数字分别对应的模板音频。当给定一个需要识别的音频时，使用DTW计算这个音频与10个模板音频的距离，从中挑选距离最短的一个，对应的模板的数字就是识别结果。由于语音本身的时变特性，不可能要求识别的音频和模板音频时长保持一致，对于时长不匹配的问题，DTW通过引入warping function Tx(t), Ty(t),t=1,2,...T.对测试音频和模板音频进行非线性对齐。给定样本X和Y，有很多可选的对齐方式，随着Tx和Ty 的增加，可选的对齐方式数量成指数级增长。DTW使用Dynamic Programming（DP），极大地缩小搜索空间，通过保存历史信息，找到最优路径。但DTW方法可扩展性比较差，当语音识别的词汇量增加时，模板的数量也就随之增加，当识别任务变为大规模语音识别时，DTW计算测试音频与每个模板之间的距离已经变得不切实际。由此，隐马尔科夫模型替代了DTW，一直沿用至今。

隐马尔科夫模型是概率模型，用来描述由马尔科夫链随机生成不可观测的状态序列的过程。状态序列里的每个状态都会在当前时刻t生成一个观测，由此组成观测序列。隐马尔可夫模型遵从两个基本假设：1）隐马尔可夫模型在任意t时刻的状态只与前一时刻的状态有关，与其它任意时刻的状态及观测都无关；2）任意时刻的观测只依赖于该时刻的状态，与其它状态及观测无关。基于以上的假设，隐马尔科夫模型可以由三个参数决定：1）状态转移概率；2）观测概率；3）初始状态概率。给定这三个参数，隐马尔科夫模型就定固定了。以下是隐马尔科夫模型三个参数的定义：设S是所有可能的状态的集合，M是所有可能的观测的集合。$$S={s_1,s_2,...,s_H}, \quad M={m_1,m_2,...,m_K}$$，其中，H为所有可能的状态数，K为所有可能的观测数。现有长度为T的状态序列$\Theta$，以及状态序列对应的观测序列$\Phi$。$$\Theta=(\theta_1,\theta_2,...,\theta_T), \quad \Phi=(\phi_1,\phi_2,...,\phi_N)$$则状态转移概率矩阵:
\begin{eqnarray}
  A=[a_{ij}]_{H\times H} \\
  a_{ij} = P(\theta_{t+1}=s_j | \theta_t=s_i), i=1,2,...,H; j=1,2,...,H
\end{eqnarray}
$a_{ij}$代表t时刻的状态$s_i$在t+1时刻跳转到$s_j$的概率。观测概率矩阵可以用B来表示:
\begin{eqnarray}
  B=[b_j(n)]_{H\times K} \\
  b_j(n) = P(\phi_t=m_n | \theta_t=s_j), n=1,2,...,K; j=1,2,...,H
\end{eqnarray}
$b_j(n)$代表t时刻处于状态j的前提下，观测到$\phi_n$的概率。初始状态概率向量可以用$\Psi$表示：
\begin{eqnarray}
  \Psi=(\psi_1,\psi_2,...,\psi_i,...,\psi_H) \\
  \psi_i = P(\theta_1=s_i), \quad i=1,2,...,H
\end{eqnarray}
$\psi_i$代表初始时刻处于状态$s_i$的概率。

隐马尔科夫模型可以由以上定义的状态转移概率矩阵A，观测概率矩阵B，以及初始状态概率向量$\Psi$完全确定。至此，隐马尔科夫模型可以表示为$\Pi=(A,B,\Psi)$。定义完模型之后，接下来引出隐马尔科夫模型的三个基本问题：

1) 概率计算问题。给定模型$\Pi=(A,B,\Psi)$及观测序列$\Phi=(\phi_1,\phi_2,...,\phi_N)$，计算在模型$\Pi$下观测到$\Phi$这一观测序列的概率；

2) 解码问题。给定模型$\Pi=(A,B,\Psi)$及观测序列$\Phi=(\phi_1,\phi_2,...,\phi_N)$，求解最有可能的状态序列$\Theta$；

3) 模型参数训练问题。已知观测序列$\Phi$，计算模型参数$(A,B,\Psi)$，使得似然概率$P(\Phi|\Pi)$最大。

对应到语音识别中，隐马尔科夫模型被用来对连续的特征序列建模。假设现在有一个英文数字0-9的识别任务，如何利用隐马尔科夫模型识别数字呢？根据2.1.1的MFCC特征提取过程，一个1s的音频，如果每10ms移动一帧，那么会得到将近100个39维的MFCC特征向量。这些特征向量对应的就是观测序列$\Phi=(\phi_1,\phi_2,...,\phi_N)$。在语音识别中，首先会给声音的最小单位建立隐马尔科夫模型，对于英语来说，最小单位就是音素。而词表中的每个单词都能由音素构成。比如‘one’这个单词，可以用W, AH, N这三个音素来表示其发音。对应音素的隐马尔科夫模型包含三个状态，分别对应发音的起始阶段、中间阶段和结束过程。表示单词的隐马尔科夫模型就是用组成其发音的音素的隐马尔科夫模型串联起来的。下图表示了one这个单词对应的隐马尔科夫模型，其它9个数字对应的隐马尔科夫模型同理。{\color{red}{\large 插入隐马尔科夫模型图片}}。有了词表里所有单词对应的隐马尔科夫模型$\Pi_i,i=0,1,...9$，同时也有了观测序列$\Phi=(\phi_1,\phi_2,...,\phi_N)$，那么如何识别当前音频到底是哪一个数字呢？这就对应到了隐马尔科夫模型的第一个基本问题，概率计算问题。我们需要分别计算$P(\Phi|\Pi_i), i=0,1,...,9$这10个似然概率。其中似然概率最大的模型对应的数字就是识别结果。\begin{equation}result=\underset{i}{\operatorname{argmax} P(\Phi|\Pi_i)} \end{equation}
\subsection{语言模型}
根据公式2-3，可以看到目标函数包含$P(\omega)$这一项。$\omega$表示单词的序列，$P(\omega)$代表这一单词序列的先验概率。为什么要加入这一项先验概率呢？根据2.1.2，我们已经知道声学模型可以计算似然概率$P(O|\Omega)$，其中$O$代表观测向量，$\Omega$代表单词序列。但是如果仅凭声学模型得到的结果去判断最可能的单词序列$\Omega$的话，没办法处理语言中同音字的情况。比如英语里的'good four me'和'good for me'，这两组句子从发音上将几乎没有差别，因此通过声学模型得到的结果也几乎相同。但是，'good for me'更符合英语的规则和使用习惯，$P('good\quad for\quad me')$要明显大于$P('good\quad four\quad me')$。因此，在目标函数中加入语言规则的先验知识，能更好地区分从声学角度上比较容易混淆的语音。然而，语言的规则并不是通过显式地规定语法规则，而是通过统计模型。比较常用的语言模型为N-gram模型。

N-gram模型是一种统计模型，由于其简单有效，在语音识别中得到了广泛的应用。语言模型的目标为计算字符串$\omega$作为一个句子出现的概率$P(\omega)$。对于一个由$m$个字符组成的序列$\omega=c_1 c_2...c_m$而言，
\begin{eqnarray}
  P(\omega) = P(c_1)P(c_2|c_1)P(c_3|c_1 c_2)...P(c_m|c_1 c_2...c_{m-1}) \\
          & = \prod\limits_{i=1}^m P(c_i|c_1...c_{i-1})
\end{eqnarray}
上式中，产生第$i$个字符$c_i$的概率是由已经产生的$i-1$个字符$c_1 c_2...c_{i-1}$决定的。字符序列$c_1 c_2...c_{i-1}$可以看成是$c_i$的历史信息。对于这种计算方法，假设词表长度为N，那么$c_1 c_2...c_{i-1}$将会有$N^{i-1}$种可能，当$i$增加到一定程度，几乎已经无法计算$P(c_i|c_1...c_{i-1})$。因此，需要对模型做出一定的简化。N-gram模型假设，对于预测当前字符$c_i$产生的概率，只需要参考前N-1个已经出现的字符$c_{i-n+1} c_{i-n+2}, c_{i-1}$，而不需要考虑所有的历史信息。这样一来，$P(\omega)$的计算公式就变为：
\begin{equation}P(\omega) = \prod\limits_{i=1}^m P(c_i|c_{i-n+1}^{i-1})\end{equation}
通常N的取值不能太大，否则依旧会出现计算量过大导致参数无法估计的问题，所以通常选取N=1,2,3。n=1时，第i个词出现的概率独立于前面的词，称为unigram；n=2时，第i个词出现的概率只与前一个词$c_(i-1)$有关，称为bigram；n=3的情况最多，即第n个词出现的概率与前两个词$c_(i-1)^(i-2)$有关，记为trigram。因为句子中每个词出现的概率可以通过统计语料中该词出现的次数得到。以unigram为例，为了计算条件概率$P(c_i|c_{i-1})$，可以通过统计二元词序列出现的次数来计算得到，计算公式如下：
\begin{equation}P(c_i|c_{i-1})=\frac{count(c_{i-1} c_i)}{\sum\limits_{c_i} count(c_{i-1} c_i)})\end{equation}
在使用N-gram语言模型计算出现的概率时，由于训练数据稀疏，对于很多正常的句子$\omega$，其出现的概率$P(\omega)=0$。这就意味着，在语音识别的应用中，即使句子$\omega$在声学模型下的打分很高，但是由于$P(\omega)=0$，使得识别结果永远不会出现$\omega$。解决这种错误的一种典型的方法为平滑方法，其基本思想为提高低概率，降低高概率，从而解决零概率的问题。实际应用中最简单的平滑方法之一为加法平滑方法，基本思想为：假设每一个n元语法发生的次数比实际统计的次数多$\delta$次，$0\leqslant \delta \leqslant 1$。此时：
\begin{equation}P_{add}(c_i|c_{i-n+1}^{i-1})=\frac{\delta+count(c_{i-n+1}^{i})}{\delta \lvert V\rvert\ + \sum\limits_{c_i} count(c_{i-n+1}^i)}\end{equation}
其中，$\lvert V\rvert$表示词汇表单词的个数。此外，常用的平滑方法还有古德-图灵估计法，Katz平滑方法，Jelinek-Mercer平滑方法，绝对减值法，Kneser-Ney平滑方法等。
\subsection{发音词典}

\subsection{解码器}

\subsection{评价指标}
对于语音识别任务来说，比较常用评价指标为词级别错误率(Word Error Rate)。对于语音识别的结果，总共包含三种类型的错误，分别是替换错误、删除错误、插入错误。从字面意思上来理解，替换错误指的是原本句子里的某个词在识别结果里被错误替换成其它的词；删除错误指的是原本句子里出现的某个词，在识别结果里被错误地去掉了；插入错误错误指的是识别结果里出现了原本句子里没有出现过的某个词。这三种错误无论出现哪一种，都会使得识别性能下降。所以语音识别中使用的各种算法的目的，就是要尽量降低这三种错误出现的可能，也就是要降低词级别错误率。词级别错误率的公式表述如下：
\begin{equation}WER=100\times\frac{count(Deletion)+count(Insertion)+count(Substitution)}{count(Words)}\end{equation}
其中，$count(words)$代表原始句子的单词总数，$count(Deletion)$、$count(Insertion)$、$count(Substitution)$分别代表识别结果中出现删除错误、插入错误、替换错误的个数。
\section{Kaldi工具箱}
Kaldi是一款用C++实现的、基于Apache License v2.0协议的开源语音识别工具箱。相比于其它现存的语音识别工具箱，Kaldi工具箱的特点可以总结
